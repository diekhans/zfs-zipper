* eSATA disk protocols on FreeBSD
    # to attach external disk after boot:
            sudo camcontrol rescan all
            sudo camcontrol devlist
      -  which should result in something like:
      -     <WDC WD5000AADS-00S9B0 01.00A01>   at scbus8 target 0 lun 0 (pass4,ada2)
      -   then try
            sudo camcontrol start ?:0:0
            sudo camcontrol reset ?:0:0
    # to get serial number
        sudo camcontrol identify ?:0:0

    # to enable standby:
       sudo camcontrol standby ?:0:0 -t 60

* disk info on mac
# get mapping to disk devices
diskutil list
# list info for disk-info.tsv
smartctl -i /dev/disk0
* ZFS documentations
   ZFS Cheatsheet  http://www.datadisk.co.uk/html_docs/sun/sun_zfs_cs.htm


* Notes
  zfs set com.apple.mimic_hfs=on
doesn't actually help with lightroom import

* initializing a zfs RAID-1 pool backup disk
# make mirrrored pool, specify -f if it is used
zpool create -O atime=off -O compression=lz4 -O dedup=off -m /media/zackup? osprey_zackup?? mirror disk? disk??
# verify it is configured as RAID 0
zpool status osprey_zackup1a

# set sleep time on new disks (HOW? pmset is global)
pmset -a disksleep 10

# can do this to kill the header if needed
dd if=/dev/zero of=/dev/XX bs=1m count=128

* initialize a ZFS 2 volume pool backup disk

zpool create -O atime=off -O compression=lz4 -O dedup=off -m /media/zackup? osprey_zackup?? disk? disk??


* discovering pool status
   sudo zpool status osprey_backup?
   sudo zpool list osprey_backup?

* clear labels on disk so it can be reused in a new pool
    zpool labelclear /dev/ada?


* pool I/O is currently suspended error
    zpool clear $pool
  # if that doesn't work
    zpool clear -nFX $pool

  # then
    zpool import $pool


* listing snapshots
   zfs list -r -t snapshot <dataset>

* listing file systems
   zfs list -r -t filesystem <pool>

* listing properties
  zpool get -o all all <pool>
  zfs get -o all all <filesystem>

* list filesystems
  zfs list
* list snapshots
zfs list -r -t snapshot zfszipper_test_source
zfs list -ro space <pool>?
* bring online
   sudo zpool import osprey_backup?
   sudo zfs mount osprey_backup?

* take offline
   sudo zfs unmount osprey_backup?
   sudo zpool export osprey_backup?

* change mount point

* rename a pool
zpool export osprey_zackup1a
zpool import osprey_zackup1a osprey_zackup1a_no_space
zpool status -v

* first backup
    (sudo /opt/sbin/zfs-zipper --full /opt/etc/zfs-zipper.conf.py 2>&1 | mail -s "ZFS Zipper daily backup: $(hostname)" root)</dev/null &
* information
- version being run:
   sysctl -a | grep kext
* replacing drive in pool
- find new disk with
  diskutil list
  or look in /var/run/disk/by-id/  ???
- use internal disk id of old disk:
  zpool status -g
  sudo zpool replace POOL 1111111111111111111 /dev/diskXXX
  sudo zpool clear POOL

* cannot export XXX': pool I/O is currently suspended
sudo zpool clear -nFX  XXX


